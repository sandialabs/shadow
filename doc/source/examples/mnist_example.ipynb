{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST Example\n",
    "\n",
    "For this demo, we load the MNIST (handwritten digits) dataset using torchvision, define a simple convolutional \n",
    "architecture, and train a prediction model using the exponential average adversarial training technique (EAAT) with 10%\n",
    " of the MNIST labels. This example is meant as a quick-start guide and to reinforce what is provided in the \n",
    " documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch imports\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# shadow-ssml imports\n",
    "import shadow.eaat\n",
    "from shadow.utils import set_seed\n",
    "\n",
    "# helpers\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Torchvision makes it easy to load and perform standard preprocessing operations on a variety of data transforms. \n",
    "Instead of using the MNIST class for the fully-labeled training datasets, we define our own MNIST class to return \n",
    "partially labeled (labeled and unlabeled) training data. Then we define our dataset for training as the MNIST training data with 90% of the \n",
    "labels reassigned to a value to -1 using a consistent sampling seed. Lastly, we use the standard torchvision MNIST \n",
    "class test partition, keeping all labels, for evaluation of SSL classification performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "datadir = 'data'\n",
    "set_seed(0)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "\n",
    "class UnlabeledMNIST(torchvision.datasets.MNIST):\n",
    "    def __init__(self, root, train=True,\n",
    "                 transform=torchvision.transforms.ToTensor(),\n",
    "                 download=False, unlabeled_frac=0.9):\n",
    "        super(UnlabeledMNIST, self).__init__(root,\n",
    "                 train=train, transform=transform,\n",
    "                 download=download)\n",
    "        labels_to_drop = np.random.choice(len(self),\n",
    "                 size=int(len(self) * unlabeled_frac),\n",
    "                 replace=False)\n",
    "        self.targets[labels_to_drop] = -1\n",
    "\n",
    "\n",
    "dataset = UnlabeledMNIST(datadir, train=True, download=True,\n",
    "                         transform=torchvision.transforms.ToTensor())\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset, batch_size=100)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(torchvision.datasets.MNIST(\n",
    "    datadir, train=False, download=True,\n",
    "    transform=torchvision.transforms.ToTensor()),\n",
    "    batch_size=100, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset UnlabeledMNIST\n",
      "    Number of datapoints: 60000\n",
      "    Split: train\n",
      "    Root Location: data\n",
      "    Transforms (if any): ToTensor()\n",
      "    Target Transforms (if any): None\n"
     ]
    }
   ],
   "source": [
    "print(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we define our parameter dictionary for non-default parameters used by the EAAT technique. For example, we rarely \n",
    "require more than one power iteration to compute the adversarial direction. Likewise, we maintain defaults for student \n",
    "and teacher noise. As a reminder, EAAT is a combination of exponential averaging, which uses random gaussian \n",
    "perturbations, and adversarial training, which uses data-specific adversarial perturbations. If your dataset may \n",
    "benefit from additive noise AND adversarial perturbations, the EAAT parameters {student_noise, teacher_noise} would be \n",
    "included in the model and in hyperparameter searches. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "eaatparams = { \n",
    "        \"xi\": 1e-8,\n",
    "        \"eps\": 2.3,\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we define a simple convolutional architecture with Relu and Dropout. Forward, in this case, does not return \n",
    "Softmax on the final layer. Typically the loss for each technique implements Softmax scaling. We then instantiate the\n",
    " model and the optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, 3, 1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, 1)\n",
    "        self.dropout = nn.Dropout2d(0.1)\n",
    "        self.fc1 = nn.Linear(9216, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = self.dropout(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "model = Net()\n",
    "eaat = shadow.eaat.EAAT(model=model, **eaatparams)\n",
    "optimizer = optim.SGD(eaat.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, we have partially-labeled training data available through our train_loader and fully-labeled \n",
    "testing data from test_loader. We have initialized a model, specified that we plan to use EAAT, and passed the EAAT \n",
    "parameters to the model. The last step is to train the model. The loss function for the SSL techniques implemented here is a \n",
    "combination of the loss on labeled data, where we typically use cross-entropy, and the technique-specific consistency \n",
    "cost. We specify the labeled data cost (xEnt), ignoring labels of -1, which we used as the unlabeled target values. \n",
    "During training, we give the labeled loss and the consistency loss equal weight by simply adding them together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 loss: 1.659409761428833\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1 loss: 1.2583383321762085\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 2 loss: 1.0968275666236877\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 3 loss: 0.9439108073711395\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 4 loss: 0.8373939394950867\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 5 loss: 0.75471630692482\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 6 loss: 0.6925887763500214\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 7 loss: 0.6281741559505463\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 8 loss: 0.5970003604888916\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 9 loss: 0.5248749554157257\n"
     ]
    }
   ],
   "source": [
    "xEnt = torch.nn.CrossEntropyLoss(ignore_index=-1)\n",
    "\n",
    "eaat.to(device)\n",
    "losscurve = []\n",
    "for epoch in range(10):\n",
    "    eaat.train()\n",
    "    lossavg = []\n",
    "    for i, (data, targets) in enumerate(train_loader):\n",
    "        x = data.to(device)\n",
    "        y = targets.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        out = eaat(x)\n",
    "        loss = xEnt(out, y) + eaat.get_technique_cost(x)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        lossavg.append(loss.item())\n",
    "    losscurve.append(np.median(lossavg))\n",
    "    print('epoch {} loss: {}'.format(epoch, losscurve[-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After training, we evaluate the performance over our test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test accuracy: 96.45\n"
     ]
    }
   ],
   "source": [
    "eaat.eval()\n",
    "y_pred, y_true = [], []\n",
    "for i, (data, targets) in enumerate(test_loader):\n",
    "    x = data.to(device)\n",
    "    y = targets.to(device)\n",
    "    out = eaat(x)\n",
    "    y_true.extend(y.detach().cpu().tolist())\n",
    "    y_pred.extend(torch.argmax(out, 1).detach().cpu().tolist())\n",
    "test_acc = (np.array(y_true) == np.array(y_pred)).mean() * 100\n",
    "print('test accuracy: {}'.format(test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
